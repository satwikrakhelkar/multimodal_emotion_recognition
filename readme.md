                                                                                                                         # Multimodal Emotion Recognition

## ğŸ“Œ Objective
This project implements emotion recognition using:
- Speech-only pipeline
- Text-only pipeline
- Multimodal fusion pipeline (speech + text)

Dataset: [Toronto Emotional Speech Set (TESS)](https://www.kaggle.com/)



ğŸš€ Features
â€¢ 	Speech pipeline: Extracts acoustic features and classifies emotional tone.
â€¢ 	Text pipeline: Processes transcripts using transformerâ€‘based models.
â€¢ 	Fusion model: Combines outputs from speech and text for final emotion prediction.
â€¢ 	Visualization tools: Embedding analysis with PCA/tâ€‘SNE for interpretability.
â€¢ 	Reproducible setup: Dependencies tracked in , large models managed via Git LFS.

ğŸ“‚ Repository Structure
multimodal_emotion_recognition/
â”‚
â”œâ”€â”€ project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ speech_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py        # Training script for speech-only model
â”‚   â”‚   â”‚   â”œâ”€â”€ test.py         # Testing script for speech-only model
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ text_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py        # Training script for text-only model
â”‚   â”‚   â”‚   â”œâ”€â”€ test.py         # Testing script for text-only model
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ fusion_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py        # Training script for multimodal fusion model
â”‚   â”‚   â”‚   â”œâ”€â”€ test.py         # Testing script for multimodal fusion model
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ speech_preprocess.py  # Silence trimming, resampling
â”‚   â”‚   â”œâ”€â”€ text_preprocess.py    # Tokenization, cleaning
â”‚   â”‚
â”‚   â”œâ”€â”€ feature_extraction/
â”‚   â”‚   â”œâ”€â”€ speech_features.py    # MFCCs, spectrograms, embeddings
â”‚   â”‚   â”œâ”€â”€ text_features.py      # Word embeddings, contextual vectors
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py     # Load TESS dataset
â”‚   â”‚   â”œâ”€â”€ visualization.py      # t-SNE/PCA plots for embeddings
â”‚   â”‚   â”œâ”€â”€ metrics.py            # Accuracy, confusion matrix
â”‚
â”œâ”€â”€ Results/
â”‚   â”œâ”€â”€ speech_results.csv        # Accuracy table for speech-only
â”‚   â”œâ”€â”€ text_results.csv          # Accuracy table for text-only
â”‚   â”œâ”€â”€ fusion_results.csv        # Accuracy table for multimodal
â”‚   â”œâ”€â”€ error_analysis.md         # Document 3â€“5 failure cases
â”‚   â”œâ”€â”€ visualizations/           # Plots of emotion clusters
â”‚
â”œâ”€â”€ Report/
â”‚   â”œâ”€â”€ Assignment2_Report.pdf    # Final report with architectures, experiments, analysis
â”‚   â”œâ”€â”€ figures/                  # Any diagrams/plots used in report
â”‚
â”œâ”€â”€ requirements.txt              # All dependencies (torch, librosa, transformers, etc.)
â”œâ”€â”€ README.md                     # Setup instructions, usage, repo overview
â”œâ”€â”€ LICENSE

âš™ï¸ Installation
Clone the repository and set up the environment:
git clone https://github.com/satwikrakhelkar/multimodal_emotion_recognition.git
cd multimodal_emotion_recognition

python -m venv .venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate      # On Windows

pip install -r requirements.txt

## ğŸ“‚ Datasets
This project uses the **Toronto Emotional Speech Set (TESS)** dataset, available on Kaggle:

- [Toronto Emotional Speech Set (TESS)] (https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)

The dataset contains speech samples along with corresponding transcripts and emotion labels.

### Setup
1. Download the dataset from Kaggle.
2. Place it in the `data/`
3. Run preprocessing scripts:
   
run:
python src/preprocessing/preprocess_speech.py
python src/preprocessing/preprocess_text.pyRun preprocessing scripts:

run:
python src/preprocessing/preprocess_speech.py
python src/preprocessing/preprocess_text.py


ğŸ“Š Usage
Speechâ€‘only pipeline:
python src/models/speech_pipeline/train.py
python src/models/speech_pipeline/test.py

Textâ€‘only pipeline:
python src/models/text_pipeline/train.py
python src/models/text_pipeline/test.py

Fusion pipeline:
python src/models/fusion_pipeline/train.py --config configs/fusion.yaml
python src/models/fusion_pipeline/test.py --model Results/fusion_model.pth


ğŸ“ˆ Results
Performance on heldâ€‘out test sets:

Model Variant        Accuracy    Notes
------------------------------------------------------------
Speech-only          15.38%      Poor convergence, weak classification
Text-only            28.57%      Undertrained, limited contextual learning
Fusion (Speech+Text) 100.00%     Perfect separation, strong multimodal benefit

- Accuracy tables and error analysis are available in Results/.
- Confusion matrices and metrics are in metrics/.
- PCA/tâ€‘SNE plots are in plots/.

ğŸ› ï¸ Tech Stack
â€¢ 	Python 3.9+
â€¢ 	PyTorch for deep learning
â€¢ 	HuggingFace Transformers for text modeling
â€¢ 	Git LFS for large model files

ğŸ“Œ Notes
â€¢ 	Large files (, ) are tracked via Git LFS.
Evaluators must install Git LFS before cloning to access full model files:
git lfs install
git clone https://github.com/satwikrakhelkar/multimodal_emotion_recognition.git
git lfs pull

â€¢ 	Datasets are not included due to size; please add them manually in .

ğŸ‘¨â€ğŸ’» Author
Satwik Rakhelkar
Finalâ€‘year Electronics & Communication Engineering student, Matrusri Engineering College.
Internship experience at ISRO and Vishwam.AI, with expertise in AI/ML, robotics, and embedded systems.





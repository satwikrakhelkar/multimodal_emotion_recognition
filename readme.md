# Multimodal Emotion Recognition

## ğŸ“Œ Overview
This project implements a **multimodal emotion recognition pipeline** that fuses speech, text, and visual modalities to classify human emotions.  
It combines deep learning models for each modality and integrates them into a fusion model for improved accuracy and robustness.

## ğŸš€ Features
- **Speech pipeline**: Extracts acoustic features and classifies emotional tone.
- **Text pipeline**: Processes transcripts using transformer-based models.
- **Fusion model**: Combines outputs from speech and text for final emotion prediction.
- **Visualization tools**: Embedding analysis with PCA/t-SNE for interpretability.
- **Reproducible setup**: Requirements tracked in `requirements.txt`, large models managed via Git LFS.

## ğŸ“‚ Repository Structure

multimodal_emotion_recognition/
â”œâ”€â”€ data/                     # Raw datasets (external, not versioned)
â”‚   â”œâ”€â”€ speech/               # Speech/audio data
â”‚   â”œâ”€â”€ text/                 # Text transcripts
â”‚   â””â”€â”€ fusion/               # Preprocessed multimodal data
â”‚
â”œâ”€â”€ src/                      # Core source code
â”‚   â”œâ”€â”€ preprocessing/        # Data cleaning & feature extraction
â”‚   â”œâ”€â”€ models/               # Model architectures
â”‚   â”‚   â”œâ”€â”€ speech_pipeline/  # Speech emotion model
â”‚   â”‚   â”œâ”€â”€ text_pipeline/    # Text emotion model
â”‚   â”‚   â””â”€â”€ fusion_model/     # Fusion logic
â”‚   â”œâ”€â”€ training/             # Training scripts
â”‚   â”œâ”€â”€ evaluation/           # Evaluation scripts & metrics
â”‚   â””â”€â”€ visualization/        # PCA/t-SNE plots, embedding analysis
â”‚
â”œâ”€â”€ Results/                  # Deliverables
â”‚   â”œâ”€â”€ metrics/              # Accuracy tables, confusion matrices
â”‚   â”œâ”€â”€ plots/                # Graphs, PCA/t-SNE visualizations
â”‚   â””â”€â”€ reports/              # Final evaluation reports
â”‚
â”œâ”€â”€ configs/                  # Experiment configs (YAML/JSON)
â”‚
â”œâ”€â”€ models/                   # Saved weights (Git LFS tracked)
â”‚   â”œâ”€â”€ speech_model.pth
â”‚   â”œâ”€â”€ text_model.safetensors
â”‚   â””â”€â”€ fusion_model.pth
â”‚
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ .gitignore                # Ignore rules for clean repo
## âš™ï¸ Installation
Clone the repository and set up the environment:
```bash
git clone https://github.com/satwikrakhelkar/multimodal_emotion_recognition.git
cd multimodal_emotion_recognition
python -m venv .venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate      # On Windows
pip install -r requirements.txt

î·™î·š
ğŸ“Š Usage
Train and evaluate the fusion model:
python src/train_fusion.py --config configs/fusion.yaml


Run evaluation and generate results:
python src/evaluate.py --model Results/fusion_model.pth


ğŸ“ˆ Results
- Accuracy tables and error analysis are available in Results/.
- Embedding visualizations (PCA/t-SNE) provide insights into modality fusion.
ğŸ› ï¸ Tech Stack
- Python 3.9+
- PyTorch for deep learning
- Transformers for text modeling
- Git LFS for large model files
ğŸ“Œ Notes
- Large files (*.pth, *.safetensors) are tracked via Git LFS.
- Datasets are not included due to size; please add them manually in data/.
ğŸ‘¨â€ğŸ’» Author
Developed by Satwik Rakhelkar
Final-year Electronics & Communication Engineering student, Matrusri Engineering College.
Internship experience at ISRO and Vishwam.AI, with expertise in AI/ML, robotics, and embedded systems.




Multimodal Emotion Recognition
ğŸ“Œ Overview
This project implements a multimodal emotion recognition pipeline that fuses speech, text, and visual modalities to classify human emotions.
By combining deep learning models for each modality and integrating them into a fusion model, the system achieves improved accuracy and robustness.

ğŸš€ Features
â€¢ 	Speech pipeline: Extracts acoustic features and classifies emotional tone.
â€¢ 	Text pipeline: Processes transcripts using transformerâ€‘based models.
â€¢ 	Fusion model: Combines outputs from speech and text for final emotion prediction.
â€¢ 	Visualization tools: Embedding analysis with PCA/tâ€‘SNE for interpretability.
â€¢ 	Reproducible setup: Dependencies tracked in , large models managed via Git LFS.

ğŸ“‚ Repository Structure
multimodal_emotion_recognition/
â”œâ”€â”€ data/                # Raw datasets (external, not versioned)
â”‚   â”œâ”€â”€ speech/          # Speech/audio data
â”‚   â”œâ”€â”€ text/            # Text transcripts
â”‚   â””â”€â”€ fusion/          # Preprocessed multimodal data
â”œâ”€â”€ src/                 # Core source code
â”‚   â”œâ”€â”€ preprocessing/   # Data cleaning & feature extraction
â”‚   â”œâ”€â”€ models/          # Model architectures
â”‚   â”‚   â”œâ”€â”€ speech_pipeline/
â”‚   â”‚   â”œâ”€â”€ text_pipeline/
â”‚   â”‚   â””â”€â”€ fusion_model/
â”‚   â”œâ”€â”€ training/        # Training scripts
â”‚   â”œâ”€â”€ evaluation/      # Evaluation scripts & metrics
â”‚   â””â”€â”€ visualization/   # PCA/t-SNE plots, embedding analysis
â”œâ”€â”€ Results/             # Deliverables (Git LFS tracked models)
â”‚   â”œâ”€â”€ speech_model.pth
â”‚   â”œâ”€â”€ text_model.safetensors
â”‚   â””â”€â”€ fusion_model.pth
â”œâ”€â”€ metrics/             # Accuracy tables, confusion matrices
â”œâ”€â”€ plots/               # Graphs, PCA/t-SNE visualizations
â”œâ”€â”€ reports/             # Final evaluation reports
â”œâ”€â”€ configs/             # Experiment configs (YAML/JSON)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ .gitignore           # Ignore rules for clean repo


âš™ï¸ Installation
Clone the repository and set up the environment:
git clone https://github.com/satwikrakhelkar/multimodal_emotion_recognition.git
cd multimodal_emotion_recognition

python -m venv .venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate      # On Windows

pip install -r requirements.txt


ğŸ“Š Usage
Train and evaluate the fusion model:
python src/train_fusion.py --config configs/fusion.yaml

Run evaluation and generate results:
python src/evaluate.py --model Results/fusion_model.pth


ğŸ“ˆ Results
â€¢ 	Accuracy tables and error analysis are available in .
â€¢ 	Embedding visualizations (PCA/tâ€‘SNE) provide insights into modality fusion.

ğŸ› ï¸ Tech Stack
â€¢ 	Python 3.9+
â€¢ 	PyTorch for deep learning
â€¢ 	HuggingFace Transformers for text modeling
â€¢ 	Git LFS for large model files

ğŸ“Œ Notes
â€¢ 	Large files (, ) are tracked via Git LFS.
Evaluators must install Git LFS before cloning to access full model files:
git lfs install
git clone https://github.com/satwikrakhelkar/multimodal_emotion_recognition.git
git lfs pull

â€¢ 	Datasets are not included due to size; please add them manually in .

ğŸ‘¨â€ğŸ’» Author
Satwik Rakhelkar
Finalâ€‘year Electronics & Communication Engineering student, Matrusri Engineering College.
Internship experience at ISRO and Vishwam.AI, with expertise in AI/ML, robotics, and embedded systems.
